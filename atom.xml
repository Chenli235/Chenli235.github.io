<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chenli235.github.io</id>
    <title>Chen Li</title>
    <updated>2020-04-15T07:33:22.298Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chenli235.github.io"/>
    <link rel="self" href="https://chenli235.github.io/atom.xml"/>
    <subtitle>UNC &amp; NCSU Biomedical Imaging Lab</subtitle>
    <logo>https://chenli235.github.io/images/avatar.png</logo>
    <icon>https://chenli235.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Chen Li</rights>
    <entry>
        <title type="html"><![CDATA[Using HPC to train a toy deeplearning network]]></title>
        <id>https://chenli235.github.io/post/using-hpc-to-train-a-toy-deeplearning-network/</id>
        <link href="https://chenli235.github.io/post/using-hpc-to-train-a-toy-deeplearning-network/">
        </link>
        <updated>2020-04-08T16:02:35.000Z</updated>
        <content type="html"><![CDATA[<p>This article is a summary of how to use HPC to train neural network, the sources are coming from the <a href="https://projects.ncsu.edu/hpc/main.php/">offical webiste</a>.</p>
<h1 id="1-get-access-and-log-into-the-system">1. Get access and log into the system</h1>
<ol>
<li>Ask your advisor to request the access for you</li>
<li>(For windows)Download and install <a href="https://mobaxterm.mobatek.net/download-home-edition.html">MobaXterm Home Edition</a><br>
open the software and type <code>apt-get install nano</code>, which is a small editor to edit files in hpc.</li>
<li>type <code>ssh unityID@login.hpc.ncsu.edu</code> for logging in.</li>
<li>learn some basic linux command like pwd, ls, cd, mkdir, cat, less, cp, rm</li>
</ol>
<h1 id="2-know-the-important-folder-in-hpc">2. Know the important folder in HPC</h1>
<ol>
<li>Home directory, type <code>cd /home/user_name</code>, because each account only have 1 GB quota, so I do not recommend put dataset or project in this folder.</li>
<li><strong>Scratch directory</strong>, type 'cd /share/group_name' to enter this scratch directory, because it has 10TB of quota, so we put our project  and run projects here. Besides, remember to make your own folder under the directory, <code>mkdir user_name</code>. Because the files will be erased in 30 days, tranfer the data into local michine after training.</li>
<li>Mass storage directory, type <code>cd /ncsu.volume1/lsmsmart</code> to enter. It has 1TB quota on this directory. It can be used for saving files but it is not connected with compute nodes.</li>
<li><strong>Directory for user maintained software</strong>, type <code>cd /usr/local/usrapps/group_name</code>. This apace is used to install deeplearning environment, but  I haven't get the space yet. I install the environment under <strong>scartch directory</strong>.</li>
</ol>
<h1 id="3-transferring-files">3. Transferring Files</h1>
<p>we can use SCP or SFTP to transfer files, but I would like to use Globus. Globus is a web based file transfer application that allows resilient, unattended file transfers between two Globus endpoints.<br>
we should make sure the code can run well on local machine before transferring them.<br>
there is the <a href="https://www.youtube.com/watch?v=S8_-mmAZyxQ&amp;feature=youtu.be">video Turial</a> about how to use it. The transfer webpage looks like this:<br>
<img src="https://chenli235.github.io/post-images/1586929146788.png" alt="" loading="lazy"><br>
After the transfer is done, for my code, it is under &quot;/share/lsmsmart/cli38/pythonprojects/&quot;,<br>
<img src="https://chenli235.github.io/post-images/1586929876060.png" alt="" loading="lazy"></p>
<h1 id="4-loading-and-initializing-conda-and-create-conda-environment">4. Loading and Initializing Conda, and create conda environment</h1>
<p>This part is illustrated very clearly at the <a href="https://projects.ncsu.edu/hpc/Software/Apps.php?app=Conda">website</a>. Besides, we don't have the user maintained software space. so we need to replace <code>conda create --prefix /usr/local/usrapps/[your_path]/env_ABC AAA BBB CCC</code> with <code>conda create --prefix /share/lsmsmart/unityID/[your_path]/pythonDL-gpu python=3.7</code><br>
After create the environment, type <code>conda activate /share/lsmsmart/cli38/envs/pythonDL-gpu</code> to activate the environment and install necessary pytorch packages.<br>
For example, type <code>conda install pytorch torchvision cudatoolkit=9.0 -c pytorch</code> to install pytorch gpu environment.</p>
<h1 id="5-submit-a-batch-job">5. Submit a batch job</h1>
<p>The cluster is designed to run computationally intensive jobs on compute nodes. Running resource intensive jobs on the login nodes is not permitted. The user must submit a batch script or request an interactive session via the job scheduler.<br>
I will show a example of batch script for illustration.<br>
type <code>nano submit.csh</code> under your source code directory.<br>
type the following content in the submit.csh</p>
<pre><code class="language-Shell">#!/bin/tcsh
#BSUB -n 1
#BSUB -w 1:30
#BSUB -R &quot;span[hosts=1]&quot;
#BSUB -q
#BSUB -R select[p100]
#BSUB -gpu &quot;num=1:mode=shared:mps=yes&quot;
#BSUB -x

#BSUB -o out
#BSUB -e err

module load conda
module load cuda/9.0
conda activate /share/lsmsmart/cli38/envs/pythonDL-gpu
python train.py
conda deactivate
</code></pre>
<p>-n cores or tasks, for small task, n=1 is enough, for heavy jobs or GPU, we can try -n 8 or -n 16. Sometimes the code will automatically try to use as many as cores possible, and it may slowed down other users, So you should use #BSUB -x for training on GPU. <strong>you will get warning or forced termination(maybe) if you use it inappropriately.</strong> If the status is awalys PEND, delete <code>#BSUB -x</code> or use other GPU.<br>
-w 1:30 set the runtime limit of the job 1 hour 30 minutes . For GPU jobs, the maximum time is 10 days.<br>
-q gpu means we assign the job to a gpu node.<br>
-R select[gtx1080] means we use gtx1080 gpu node.<br>
-gpu &quot;num=1:mode=shared:mps=yes&quot; is nvidia's multiple process service(mps), only supported by RTX 2080, GTX 1080, P100, K20m<br>
-x means you use the node exclusively<br>
-o and -e is the output file and error file<br>
type <code>bsub &lt; submit.csh</code> to submit job.<br>
use <code>bpeek</code> to monitor the process of training<br>
<img src="https://chenli235.github.io/post-images/1586935187216.png" alt="" loading="lazy"><br>
<strong>Tips: the setup of batch script may be different under different conditions, if the server is busy, we may need to change the environment to use other low performance GPUs.</strong></p>
<h1 id="6-monitor-the-jobs">6. Monitor the jobs</h1>
<p><code>bjobs</code> can monitor the status of current submitted jobs<br>
<code>bkill 0</code> or <code>bkill JOBID</code> to kill all or specific job<br>
<code>bpeek</code>  to monitor the standard output of runing jobs</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Journal club2: Deconvolution of light sheet microscopy recordings]]></title>
        <id>https://chenli235.github.io/post/journal-club2-deconvolution-of-light-sheet-microscopy-recordings/</id>
        <link href="https://chenli235.github.io/post/journal-club2-deconvolution-of-light-sheet-microscopy-recordings/">
        </link>
        <updated>2020-03-22T04:37:00.000Z</updated>
        <content type="html"><![CDATA[<p>This paper can be found in <a href="https://www.nature.com/articles/s41598-019-53875-y">there</a></p>
<h1 id="introduction-of-psf">Introduction of PSF</h1>
<p><img src="https://chenli235.github.io/post-images/1585286735808.PNG" alt="" loading="lazy"><br>
The image formation is described above. the microscope image can be thought as the real image f convolved with PSF function h, which shows blurring effect, and add some background noises <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span>.<br>
The PSF function can be measured in two ways:</p>
<ul>
<li>Calculating a theoretical PSF, obtained from microscopic parameters</li>
<li>Recording of fluorescent beads</li>
</ul>
<h1 id="the-deconvolutiong-algorithm">The deconvolutiong algorithm</h1>
<p>Deconvolution is a mathematical operation used in Image Restoration to recover an image that is degraded by a process than can be described with a Convolution.<br>
<img src="https://chenli235.github.io/post-images/1585284966915.PNG" alt="" loading="lazy"><br>
The algorithm they use is Richardson–Lucy algorithm.</p>
<h1 id="result-from-the-paper">Result from the paper</h1>
<figure data-type="image" tabindex="1"><img src="https://chenli235.github.io/post-images/1585286234203.PNG" alt="" loading="lazy"></figure>
<h1 id="advantages">Advantages</h1>
<ol>
<li>Open source packages, easy to use.</li>
<li>No need to calculate the PSF function, just need the parameters of the microscope.</li>
<li>Showing good results on light sheet microscope.</li>
</ol>
<h1 id="result-from-cleared-bone-data">Result from cleared bone data</h1>
<p><img src="https://chenli235.github.io/post-images/1585285215106.PNG" alt="single image" title="single image Left: raw image, Right: deconv image" loading="lazy">single image Left: raw image, Right: deconv image</div><br>
<img src="https://chenli235.github.io/post-images/1585286009835.PNG" alt="z projection image" loading="lazy">z projection image Left: raw imageRight: deconv image</div></p>
<p>From the result, we notice that when using on one single image, the image will show some pepper noise due to the magnifing of noises, however, we apply the algorithm on whole z-stack and get the z-projection image, the noises will be averaged and gone.</p>
<h1 id="refrence">Refrence</h1>
<blockquote>
<p>Becker, Klaus, et al. &quot;Deconvolution of light sheet microscopy recordings.&quot; Scientific reports 9.1 (2019): 1-14.</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mouse brain image using light sheet microscope]]></title>
        <id>https://chenli235.github.io/post/mouse-brain-image-using-light-sheet-microscope/</id>
        <link href="https://chenli235.github.io/post/mouse-brain-image-using-light-sheet-microscope/">
        </link>
        <updated>2020-03-20T01:34:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="hardware-setup">Hardware setup</h1>
<p><img src="https://chenli235.github.io/post-images/1585272993095.PNG" alt="" loading="lazy"><br>
<img src="https://chenli235.github.io/post-images/1585273628791.PNG" alt="" loading="lazy"></p>
<h1 id="mouse-brain-image">Mouse brain image</h1>
<p><img src="https://chenli235.github.io/post-images/1585275613220.PNG" alt="" loading="lazy"><br>
some common problem with light sheet microscope</p>
<ol>
<li>Intensity discontinuity<br>
<img src="https://chenli235.github.io/post-images/1585275853344.PNG" alt="" loading="lazy"><br>
it is due to the light bleaching and scattering.</li>
<li>Light Peneration issues (when using only one light path)<br>
<img src="https://chenli235.github.io/post-images/1585275983813.PNG" alt="" loading="lazy"><br>
<img src="https://chenli235.github.io/post-images/1585276033521.PNG" alt="" loading="lazy"></li>
</ol>
<h1 id="end">end</h1>
<p>it is my first experience with light sheet microscope, and I know the quality of image is not good enough. I will try different ways to improve the image quality and processing speed.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Journal club1: Assessing microscope image focus quality with deep learning]]></title>
        <id>https://chenli235.github.io/post/articles/</id>
        <link href="https://chenli235.github.io/post/articles/">
        </link>
        <updated>2020-03-19T21:02:25.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://chenli235.github.io/post-images/1585256740222.PNG" alt="" loading="lazy"></figure>
<h1 id="intereting-paper-about-assessing-microscope-image-quality">intereting paper about assessing microscope image quality</h1>
<p>this is the <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2087-4">paper</a><br>
The advantage of the proposed method:</p>
<ol>
<li>Create dataset using only in-focus images, get out-of focus images by PSF.</li>
<li>we can generate dataset of different level of focus quality by moving focus distance.</li>
<li>directly neural network and source code are provided in this article.</li>
<li>the whole image can be divided into many small image pathes,  which accelerate the training speed.</li>
<li>the method has a imageJ plugin.</li>
<li>Data augmentation to handle noises and offset images</li>
</ol>
<h1 id="implementation">Implementation</h1>
<ol>
<li>Create an dataset of images consisting of both infocus and out of focus images of U2OS cancer cells with Hoechst stain.</li>
<li>Then defocus the infocus images by applying a convolution with the PSF(point spread function) by varying z in 2um increments.<br>
<img src="https://chenli235.github.io/post-images/1585258360693.PNG" alt="PSF" loading="lazy"><br>
J0 is the Bessel function, wavelength,numerical aperture, which are related with actual setup.</li>
<li>Apply Poisson noise, accounting for sensor offset.</li>
<li>Randomly pick 84$\times<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>84</mn><mi>i</mi><mi>m</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>c</mi><mi>r</mi><mi>o</mi><mi>p</mi><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>h</mi><mi>e</mi><mn>696</mn></mrow><annotation encoding="application/x-tex">84 image crops of the 696</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord">8</span><span class="mord">4</span><span class="mord mathdefault">i</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">e</span><span class="mord mathdefault">c</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord">6</span><span class="mord">9</span><span class="mord">6</span></span></span></span>\times$520 original size image for training, for each patch, has 11 defocus levels.<br>
<img src="https://chenli235.github.io/post-images/1585260331255.PNG" alt="good image and defocused image" loading="lazy"></li>
</ol>
<p>the neural network model architecture and some of the result<br>
<a href="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-018-2087-4/MediaObjects/12859_2018_2087_Fig2_HTML.gif?as=webp"></a><br>
The patch outlines have one of 11 hues denoting the predicted defocus level and increasing lightness denoting increased certainty.</p>
<h1 id="results">Results</h1>
<p><a href="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12859-018-2087-4/MediaObjects/12859_2018_2087_Fig7_HTML.gif?as=webp"></a><br>
Prediction of absolute focus quality on an unseen cell type (MCF-7 cells, from BBBC021 dataset [12]) and unseen stain, Tubulin, using our Fiji (ImageJ) [20] plugin with pre-trained TensorFlow model. A composite image montage was assembled using the center 84 × 84 patch from a randomly selected batch of 240 images. The border hues denote predicted defocus levels (red for best focus), while the lightness denotes prediction certainty.</p>
<h1 id="check-the-mouse-brain-image-using-the-imagej-plugin">Check the mouse brain image using the imageJ plugin</h1>
<figure data-type="image" tabindex="2"><img src="https://chenli235.github.io/post-images/1585268654621.PNG" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://chenli235.github.io/post-images/1585268819733.PNG" alt="" loading="lazy"></figure>
<p>From the result, we can get the conclusion that on images patch that have shape edges, the prediction looks good but if the signal is too strong or protein material, it may predict as a out-of-focus image.</p>
<h1 id="reference">Reference</h1>
<blockquote>
<p>Yang, S.J., Berndl, M., Michael Ando, D. et al. Assessing microscope image focus quality with deep learning. BMC Bioinformatics 19, 77 (2018). https://doi.org/10.1186/s12859-018-2087-4</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[About me]]></title>
        <id>https://chenli235.github.io/post/about/</id>
        <link href="https://chenli235.github.io/post/about/">
        </link>
        <updated>2019-01-22T18:09:48.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>Welcome to my blog, nice to meet you here.</p>
</blockquote>
<h2 id="i-am-a-phd-student-in-unc-ncsu-state-joint-department-of-biomedical-engineering-my-research-area-is-medical-imging-and-microscope-design">🏠 I am a PhD student in UNC &amp; NCSU State joint Department of  Biomedical Engineering. My research area is medical imging and microscope design.</h2>
<h2 id="i-would-like-to-share-my-project-and-interesting-papers-i-read">👨‍💻 I would like to share my project and interesting papers I read.</h2>
<h2 id="contact-info-cli38ncsueducom">📬 Contact info: cli38@ncsu.edu.com</h2>
]]></content>
    </entry>
</feed>