<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chenli235.github.io</id>
    <title>Chen Li</title>
    <updated>2020-03-26T21:36:40.869Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chenli235.github.io"/>
    <link rel="self" href="https://chenli235.github.io/atom.xml"/>
    <subtitle>UNC &amp; NCSU Biomedical Imaging Lab</subtitle>
    <logo>https://chenli235.github.io/images/avatar.png</logo>
    <icon>https://chenli235.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Chen Li</rights>
    <entry>
        <title type="html"><![CDATA[Assessing microscope image focus quality with deep learning]]></title>
        <id>https://chenli235.github.io/post/assessing-microscope-image-focus-quality-with-deep-learning/</id>
        <link href="https://chenli235.github.io/post/assessing-microscope-image-focus-quality-with-deep-learning/">
        </link>
        <updated>2020-03-25T21:02:25.000Z</updated>
        <content type="html"><![CDATA[<figure data-type="image" tabindex="1"><img src="https://chenli235.github.io/post-images/1585256740222.PNG" alt="" loading="lazy"></figure>
<h1 id="intereting-paper-about-assessing-microscope-image-quality">intereting paper about assessing microscope image quality</h1>
<p>this is the <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2087-4">paper</a><br>
The advantage of the proposed method</p>
<ol>
<li>just need dataset of good image and different degree of unfocus images.</li>
<li>we can generate dataset of different level of focus quality by moving focus distance.</li>
<li>directly neural network and source code are provided in this article.</li>
<li>the whole image can be divided into many small image pathes,  which accelerate the training speed.</li>
</ol>
<h1 id="implementation">Implementation</h1>
<ol>
<li>Create an dataset of images consisting of both infocus and out of focus images of U2OS cancer cells with Hoechst stain.</li>
<li>Then defocus the infocus images by applying a convolution with the PSF(point spread function) by varying z in 2um increments.<br>
<img src="https://chenli235.github.io/post-images/1585258360693.PNG" alt="PSF" loading="lazy"><br>
J0 is the Bessel function, wavelength,numerical aperture, which are related with actual setup.</li>
<li>Apply Poisson noise, accounting for sensor offset.</li>
<li>Randomly pick 84<em>84 image crops of the 696</em>520 original size image for training, for each patch, has 11 defocus levels.</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[test]]></title>
        <id>https://chenli235.github.io/post/test/</id>
        <link href="https://chenli235.github.io/post/test/">
        </link>
        <updated>2020-03-19T05:12:23.000Z</updated>
        <content type="html"><![CDATA[<p>test<br>
this is the website<a href="https://www.baidu.com">click this</a></p>
<ul>
<li>abc</li>
<li>bcd</li>
<li>cde</li>
<li>def</li>
</ul>
<ol>
<li>abc</li>
<li>bcd</li>
<li>cde</li>
<li>def</li>
</ol>
<blockquote>
<p>try this</p>
</blockquote>
<blockquote>
<blockquote>
<p>try this</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>try this</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p>try this</p>
</blockquote>
</blockquote>
<blockquote>
<p>try this</p>
</blockquote>
<hr>
<p>try <code>html</code></p>
<pre><code class="language-python">if __name__ == &quot;__main__&quot;:
    print('Hello world.')

</code></pre>
<figure data-type="image" tabindex="1"><img src="https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/smartest-dog-breeds-1553287693.jpg?crop=0.846xw:0.843xh;0.0994xw,0.0511xh&amp;resize=980:*" alt="dog" title="big dog" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://chenli235.github.io/post-images/1585254686367.jfif" alt="another dog" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[About me]]></title>
        <id>https://chenli235.github.io/post/about/</id>
        <link href="https://chenli235.github.io/post/about/">
        </link>
        <updated>2019-01-24T09:09:48.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>欢迎来到我的小站呀，很高兴遇见你！🤝</p>
</blockquote>
<h2 id="关于本站">🏠 关于本站</h2>
<h2 id="博主是谁">👨‍💻 博主是谁</h2>
<h2 id="兴趣爱好">⛹ 兴趣爱好</h2>
<h2 id="contact-info">📬 Contact info</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://chenli235.github.io/post/hello-gridea/</id>
        <link href="https://chenli235.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>